{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)  ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "# PEC 3: Noviembre 2019\n",
    "\n",
    "## Extracción de conocimiento de fuentes de datos heterogéneas mediante Spark SQL, RDDs y GraphFrames\n",
    "\n",
    "En esta práctica vamos a introducir estructuras de datos más complejas que las vistas hasta ahora, donde los campos pueden a su vez tener campos anidados. En concreto utilizaremos datos de twitter capturados en el contexto de las elecciones generales en España del 28 de Abril de 2019. La práctica está estructurada de la siguiente manera:\n",
    "- **Parte 0:** Configuración del entorno\n",
    "- **Parte 1:** Introducción a data frames estructurados y cómo operar extraer información *(1.5 puntos)*\n",
    "    - **Parte 1.1:** Importar los datos *(0.25 puntos)*\n",
    "    - **Parte 1.2:** *Queries* sobre sobre data frames complejos *(1.25 puntos)*\n",
    "        - **Parte 1.2.1:** Queries SQL *(0.5 puntos)*\n",
    "        - **Parte 1.2.2:** Queries sobre el pipeline *(0.75 puntos)*\n",
    "- **Parte 2:** Bases de datos HIVE y operaciones complejas *(3.5 puntos)*\n",
    "    - **Parte 2.1:** Bases de datos Hive *(0.25 puntos)*\n",
    "    - **Parte 2.2:** Más allá de las transformaciones SQL *(2.75 puntos)*\n",
    "        - **Parte 2.2.1:** Tweets por población  *(1.25 puntos)*\n",
    "            - **Parte 2.2.1.1:** Utilizando SQL *(0.25 puntos)*\n",
    "            - **Parte 2.2.1.2:** Utilizando RDD *(1 punto)*\n",
    "        - **Parte 2.2.2:** Contar hashtags *(1.5 puntos)*\n",
    "- **Parte 3:** Sampling *(3 Puntos)*\n",
    "    - **Parte 3.1:** Homogéneo *(1 punto)*\n",
    "    - **Parte 3.2:** Estratificado *(1.5 puntos)*\n",
    "- **Parte 4**: Introducción a los datos relacionales *(2 puntos)*\n",
    "    - **Parte 4.1:** Generar la red de retweets *(1 punto)*\n",
    "        - **Parte 4.1.1**: Construcción de la edgelist *(0.5 puntos)*\n",
    "        - **Parte 4.1.2**: Centralidad de grado *(0.5 puntos)*\n",
    "    - **Parte 4.2:** Análisis de redes utilitzando GraphFrames *(1 punto)*\n",
    "        - **Parte 4.2.1:** Crear un graph frame (0.5 puntos)\n",
    "        - **Parte 4.2.2:** Centralidad PageRank (0.5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parte 0:** Configuración del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from math import floor\n",
    "from pyspark import SparkConf, SparkContext, SQLContext, HiveContext\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_ARGS = \"--packages graphframes:graphframes:0.7.0-spark2.4-s_2.11 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[1]\")\n",
    "# Introducid el nombre de la app PEC3_ seguido de vuestro nombre de usuario\n",
    "conf.setAppName(\"PEC3_laurivsan\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 1:** Introducción a data frames estructurados i operaciones sobre ellos.\n",
    "\n",
    "Como ya se ha mencionado, en esta práctica vamos ha utilizar datos de Twitter que recolectamos durante las elecciones generales en España del 28 de abril de 2019. Como veremos, los tweets tienen una estructura interna bastante compleja que hemos simplificado un poco en esta práctica.\n",
    "\n",
    "### **Parte 1.1:** Importar los datos\n",
    "\n",
    "Lo primero que vamos ha aprender es cómo importar este tipo de datos a nuestro entorno. Uno de los tipos de archivos más comunes para guardar este formato de información es [la estructura JSON](https://en.wikipedia.org/wiki/JSON). Esta estructura permite guardar información en un texto plano de diferentes objetos siguiendo una estructura de diccionario donde cada campo tiene asignado una llave y un valor. La estructura puede ser anidada, o sea que una llave puede tener como valor otra estructura tipo diccionario.\n",
    "\n",
    "Spark SQL permite leer datos de muchos formatos diferentes (como recordareis de la anterior práctica donde leímos un fichero CSV). En esta ocasión, se os pide que leáis un fichero JSON de la ruta ```/aula_M2.858/data/tweets28a_sample.json```. Este archivo contiene un pequeño *sample*, un 0.1% de la base de datos completa (en un siguiente apartado veremos cómo realizar este *sampleado*). En esta ocasión no se os pide especificar la estructura del data frame ya que la función de lectura la inferirá automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset contains 27268 tweets\n"
     ]
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "tweets_sample = sqlContext.read.json(\"/aula_M2.858/data/tweets28a_sample.json\")\n",
    "\n",
    "print(\"Loaded dataset contains %d tweets\" % tweets_sample.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es mostrar la estructura del dataset que acabamos de cargar. Recordad que podéis obtener la información acerca de cómo está estructurado el DataTable utilizando el método ```printSchema()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- created_at: long (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: struct (nullable = true)\n",
      " |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- country_code: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- place_type: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- _id: string (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_sample.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podéis observar que la estructura del tweet contiene múltiples campos anidados. Teneis que familiarizaros con esta estructura ya que será la que utilizaremos durante toda la práctica. Recordad también que no todos los tweets tienen todos los campos, como por ejemplo la ubicación (campo ```place```). Cuando esto pasa el campo pasa a ser ```NULL```. Podéis ver mas información sobre este tipo de datos en [este enlace](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### **Parte 1.2:** *Queries* sobre sobre data frames complejos\n",
    "\n",
    "En la anterior práctica hemos visto cómo hacer consultas sobre un dataset muy simple utilizando sentencias *SQL*. En esta parte vamos a refrescar los conceptos utilizados en la práctica anterior introduciendo algunos conceptos más avanzados y una nueva manera de trabajar sobre data tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 1.2.1:** Queries SQL\n",
    "\n",
    "Como recordaréis de la parte 3 de la anterior PEC, el primer paso consiste en registrar la tabla en el contexto SQL comprobando primero si existe y borrándola en el caso que sea así. En este apartado se os pide que registréis la tabla ```tweets_sample``` que acabamos de cargar en el contexto sql bajo el mismo nombre ```tweets_sample```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"DROP TABLE IF EXISTS tweets_sample\")\n",
    "sqlContext.registerDataFrameAsTable(tweets_sample, \"tweets_sample\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se os pide que creeis una tabla ```users_agg``` con [la información agregada](https://www.w3schools.com/sql/sql_groupby.asp) de los usuarios que tengan definido su idioma (```user.lang```) como español (```es```). En concreto se os pide que la tabla contenga las siguientes columnas:\n",
    "- **screen_name:** nombre del usuario\n",
    "- **friends_count:** número máximo (ver nota) de personas a las que sigue\n",
    "- **tweets:** número de tweets realizados\n",
    "- **followers_count:** número máximo (ver nota) personas que siguen al usuario.\n",
    "\n",
    "El orden en el cual se deben mostrar los registros es orden descendente acorde al número de tweets.\n",
    "\n",
    "***Nota:*** es importante que os fijéis que el nombre de *friends* i *followers* puede diferir a lo largo de la adquisición de datos. En este caso vamos ha utilizar la función de agregación ```MAX``` sobre cada uno de estos campos para evitar segmentar el usuario en diversas instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+------+---------------+\n",
      "|    screen_name|friends_count|tweets|followers_count|\n",
      "+---------------+-------------+------+---------------+\n",
      "|       anaoromi|         6258|    16|           6774|\n",
      "|    RosaMar6254|         6208|    14|           6245|\n",
      "|        lyuva26|         3088|    13|           3732|\n",
      "|     carrasquem|          147|    12|            215|\n",
      "|PisandoFuerte10|         2795|    12|           1752|\n",
      "|       jasalo54|         1889|    11|            689|\n",
      "|  Angel15268471|         1053|    11|            277|\n",
      "|    DuroBelinda|         5242|     9|           5778|\n",
      "| locuspolitikus|        11261|     9|          10244|\n",
      "|   Robi45852533|         4531|     9|           2530|\n",
      "+---------------+-------------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_agg = sqlContext.sql(\"SELECT user.screen_name, max(user.friends_count) as friends_count, count(_id) as tweets, max(user.followers_count) as followers_count \\\n",
    "                            from tweets_sample where user IS NOT NULL \\\n",
    "                            group by user.screen_name order by tweets DESC\")\n",
    "users_agg.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = users_agg.first()\n",
    "assert output.screen_name == 'anaoromi' and output.friends_count == 6258 and output.tweets == 16 and output.followers_count == 6774, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginad ahora que queremos combinar la información que acabamos de generar con información acerca del número de veces que un usuario ha sido retuiteado. Para hacer este tipo de combinaciones necesitamos recurrir al [```JOIN``` de tablas](https://www.w3schools.com/sql/sql_join.asp). Primero debemos registrar la tabla que acabamos de generar en el contexto SQL. Recordad que primero debéis comprobar si la tabla existe y en caso afirmativo eliminarla. La tabla tenéis que registrarla bajo el nombre de ```user_agg```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"DROP TABLE IF EXISTS user_agg\")\n",
    "sqlContext.registerDataFrameAsTable(users_agg, \"user_agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez registrada se pide que combinéis esta tabla y la tabla ```tweets_sample``` utilizando un ```INNER JOIN``` para obtener una nueva tabla con la siguiente información:\n",
    "- ***screen_name:*** nombre de usuario\n",
    "- ***friends_count:*** número máximo de personas a las que sigue\n",
    "- ***followers_count:*** número máximo de personas que siguen al usuario.\n",
    "- ***tweets:*** número de tweets realizados por el usuario.\n",
    "- ***retweeted:*** número de retweets obtenidos por el usuario.\n",
    "- ***ratio_tweet_retweeted:*** ratio de retweets por número de tweets publicados $\\frac{retweets}{tweets}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+---------------+------+---------+---------------------+\n",
      "|    screen_name|friends_count|followers_count|tweets|retweeted|ratio_tweet_retweeted|\n",
      "+---------------+-------------+---------------+------+---------+---------------------+\n",
      "|           PSOE|        13635|         671073|     1|      155|                155.0|\n",
      "|   CiudadanosCs|        92910|         511896|     1|      117|                117.0|\n",
      "|  gabrielrufian|         4678|         641252|     2|       98|                 49.0|\n",
      "|Front_Republica|           56|          28754|     2|       78|                 39.0|\n",
      "|      JuntsXCat|          202|          88515|     1|       73|                 73.0|\n",
      "|   PartidoPACMA|         1498|         232932|     1|       63|                 63.0|\n",
      "|   pablocasado_|         4567|         238926|     1|       50|                 50.0|\n",
      "| voxnoticias_es|         2146|          29582|     1|       44|                 44.0|\n",
      "|    jordiborras|         2091|         168338|     1|       43|                 43.0|\n",
      "| RaiLopezCalvet|         7579|          13574|     1|       43|                 43.0|\n",
      "+---------------+-------------+---------------+------+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retweeted = sqlContext.sql(\"SELECT screen_name, friends_count, followers_count, tweets, count(tweets_sample.retweeted_status) as retweeted, count(tweets_sample.retweeted_status)/tweets as ratio_tweet_retweeted \\\n",
    "                            FROM user_agg INNER JOIN tweets_sample  ON user_agg.screen_name = tweets_sample.retweeted_status.user.screen_name \\\n",
    "                            GROUP BY user_agg.screen_name, friends_count, followers_count, tweets \\\n",
    "                            ORDER BY retweeted DESC\")\n",
    "\n",
    "retweeted.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = retweeted.first()\n",
    "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.ratio_tweet_retweeted == 155.0 and output.retweeted == 155, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 1.2.2:** Queries a través del pipeline\n",
    "\n",
    "Las tablas de Spark SQL ofrecen otro mecanismo para aplicar las transformaciones y obtener resultados similares a los que se obtendría aplicando una consulta SQL. Por ejemplo utilizando el siguiente pipeline obtendremos el texto de todos los tweets en español:\n",
    "\n",
    "```\n",
    "tweets_sample.where(\"lang == 'es'\").select(\"text\")\n",
    "```\n",
    "\n",
    "Que es equivalente a la siguiente sentencia SQL:\n",
    "\n",
    "```\n",
    "SELECT text\n",
    "FROM tweets_sample\n",
    "WHERE lang == 'es'\n",
    "```\n",
    "\n",
    "Podéis consultar el [API de spark SQL](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html) para encontrar más información sobre como utilitzar las diferentes transformaciones en tablas.\n",
    "\n",
    "En este ejercicio se os pide que repliquéis la query obtenida en el apartado anterior empezando por generar la tabla ```users_agg```. Podéis utilizar las transformaciones ```where```, ```select``` (o ```selectExpr```), ```groupBy```, ```count```, ```agg``` y ```orderBy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------------------------------+-----------+----------------------------------------------+\n",
      "|    screen_name|max(user.friends_count AS `friends_count`)|count(user)|max(user.followers_count AS `followers_count`)|\n",
      "+---------------+------------------------------------------+-----------+----------------------------------------------+\n",
      "|       anaoromi|                                      6258|         16|                                          6774|\n",
      "|    RosaMar6254|                                      6208|         14|                                          6245|\n",
      "|        lyuva26|                                      3088|         13|                                          3732|\n",
      "|     carrasquem|                                       147|         12|                                           215|\n",
      "|PisandoFuerte10|                                      2795|         12|                                          1752|\n",
      "|       jasalo54|                                      1889|         11|                                           689|\n",
      "|  Angel15268471|                                      1053|         11|                                           277|\n",
      "|    DuroBelinda|                                      5242|          9|                                          5778|\n",
      "| locuspolitikus|                                     11261|          9|                                         10244|\n",
      "|   Robi45852533|                                      4531|          9|                                          2530|\n",
      "+---------------+------------------------------------------+-----------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = tweets_sample.where(\"user IS NOT NULL\").select(\"user\")\n",
    "\n",
    "users_agg = users.groupBy(\"user.screen_name\")\\\n",
    "                 .agg({\"user.followers_count\": \"max\", \"user.friends_count\": \"max\", \"user\": \"count\"})\\\n",
    "                 .orderBy(\"count(user)\",ascending=0)\n",
    "\n",
    "users_agg.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si os fijáis veréis que el nombre de las columnas no corresponde con el obtenido anteriormente, podéis cambiar el nombre de una columna determinada utilizando la transformación ```withColumnRenamed```. Cambiad el nombre de las columnas para que coincidan con el apartado anterior y guardadlas en una variable ```user_agg_new```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+------+---------------+\n",
      "|    screen_name|friends_count|tweets|followers_count|\n",
      "+---------------+-------------+------+---------------+\n",
      "|       anaoromi|         6258|    16|           6774|\n",
      "|    RosaMar6254|         6208|    14|           6245|\n",
      "|        lyuva26|         3088|    13|           3732|\n",
      "|     carrasquem|          147|    12|            215|\n",
      "|PisandoFuerte10|         2795|    12|           1752|\n",
      "|       jasalo54|         1889|    11|            689|\n",
      "|  Angel15268471|         1053|    11|            277|\n",
      "|    DuroBelinda|         5242|     9|           5778|\n",
      "| locuspolitikus|        11261|     9|          10244|\n",
      "|   Robi45852533|         4531|     9|           2530|\n",
      "+---------------+-------------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_agg_new = users_agg.withColumnRenamed(\"count(user)\", \"tweets\")\\\n",
    "                         .withColumnRenamed(\"max(user.friends_count AS `friends_count`)\", \"friends_count\")\\\n",
    "                         .withColumnRenamed(\"max(user.followers_count AS `followers_count`)\", \"followers_count\")\n",
    "\n",
    "users_agg_new.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = users_agg_new.first()\n",
    "assert output.screen_name == 'anaoromi' and output.friends_count == 6258 and output.tweets == 16 and output.followers_count == 6774, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cread ahora una tabla ```user_retweets``` utilizando transformaciones que contenga dos columnas:\n",
    "- ***screen_name:*** nombre de usuario\n",
    "- ***retweeted:*** número de retweets\n",
    "\n",
    "Podéis utilizar las mismas transformaciones que en el ejercicio anterior. Ordenad la tabla en orden descendente utilizando el valor de la columna ```retweeted```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|   screen_name|retweeted|\n",
      "+--------------+---------+\n",
      "|        vox_es|      299|\n",
      "|  ahorapodemos|      238|\n",
      "| Santi_ABASCAL|      238|\n",
      "|      iescolar|      166|\n",
      "| AlbanoDante76|      161|\n",
      "|          PSOE|      155|\n",
      "|AntonioMaestre|      154|\n",
      "|          KRLS|      149|\n",
      "|        boye_g|      142|\n",
      "|  CiudadanosCs|      117|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_retweets = tweets_sample.select(\"retweeted_status\")\\\n",
    "                            .groupBy(\"retweeted_status.user.screen_name\")\\\n",
    "                            .agg({\"retweeted_status\": \"count\"})\\\n",
    "                            .withColumnRenamed(\"count(retweeted_status)\", \"retweeted\")\\\n",
    "                            .orderBy(\"retweeted\", ascending=0)\n",
    "\n",
    "user_retweets.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = user_retweets.first()\n",
    "assert output.screen_name == 'vox_es' and output.retweeted == 299, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra manera de combinar dos tablas es utilizando el [metodo de tabla ```join```](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html). Combinad la información de la tabla ```users_agg_new``` y ```user_retweets``` en una nueva tabla ```retweeted``` utilizando la columna ```screen_name```. Ordenad la nueva tabla en orden descendente con el nombre de retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+------+---------------+---------------+---------+\n",
      "|    screen_name|friends_count|tweets|followers_count|    screen_name|retweeted|\n",
      "+---------------+-------------+------+---------------+---------------+---------+\n",
      "|           PSOE|        13635|     1|         671073|           PSOE|      155|\n",
      "|   CiudadanosCs|        92910|     1|         511896|   CiudadanosCs|      117|\n",
      "|  gabrielrufian|         4678|     2|         641252|  gabrielrufian|       98|\n",
      "|Front_Republica|           56|     2|          28754|Front_Republica|       78|\n",
      "|      JuntsXCat|          202|     1|          88515|      JuntsXCat|       73|\n",
      "|   PartidoPACMA|         1498|     1|         232932|   PartidoPACMA|       63|\n",
      "|   pablocasado_|         4567|     1|         238926|   pablocasado_|       50|\n",
      "| voxnoticias_es|         2146|     1|          29582| voxnoticias_es|       44|\n",
      "|    jordiborras|         2091|     1|         168338|    jordiborras|       43|\n",
      "| RaiLopezCalvet|         7579|     1|          13574| RaiLopezCalvet|       43|\n",
      "+---------------+-------------+------+---------------+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retweeted = users_agg_new.join(user_retweets, users_agg_new.screen_name == user_retweets.screen_name)\\\n",
    "                         .orderBy(\"retweeted\", ascending=0)\n",
    "\n",
    "retweeted.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = retweeted.first()\n",
    "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.retweeted == 155, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notaréis que algunos de los registros que aparecen en la tabla ```users_retweeted``` no están presentes en la tabla retweeted. Esto es debido a que, por defecto, el método aplica un inner join y por tanto solo combina los registros presentes en ambas tablas. Podéis cambiar este comportamiento a través de los parámetros de la función.\n",
    "\n",
    "Para terminar esta parte y reconstruir el resultado del apartado 1.2.1 vamos a añadir una columna ```ratio_tweet_retweeted``` con información del ratio entre retweets y tweets. Para ello debéis utilizar la transformación ```withColumn```. El resultado debe estar ordenado considerando esta nueva columna en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+------+---------------+---------------+---------+---------------------+\n",
      "|    screen_name|friends_count|tweets|followers_count|    screen_name|retweeted|ratio_tweet_retweeted|\n",
      "+---------------+-------------+------+---------------+---------------+---------+---------------------+\n",
      "|           PSOE|        13635|     1|         671073|           PSOE|      155|                155.0|\n",
      "|   CiudadanosCs|        92910|     1|         511896|   CiudadanosCs|      117|                117.0|\n",
      "|      JuntsXCat|          202|     1|          88515|      JuntsXCat|       73|                 73.0|\n",
      "|   PartidoPACMA|         1498|     1|         232932|   PartidoPACMA|       63|                 63.0|\n",
      "|   pablocasado_|         4567|     1|         238926|   pablocasado_|       50|                 50.0|\n",
      "|  gabrielrufian|         4678|     2|         641252|  gabrielrufian|       98|                 49.0|\n",
      "| voxnoticias_es|         2146|     1|          29582| voxnoticias_es|       44|                 44.0|\n",
      "|    jordiborras|         2091|     1|         168338|    jordiborras|       43|                 43.0|\n",
      "| RaiLopezCalvet|         7579|     1|          13574| RaiLopezCalvet|       43|                 43.0|\n",
      "|Front_Republica|           56|     2|          28754|Front_Republica|       78|                 39.0|\n",
      "+---------------+-------------+------+---------------+---------------+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retweeted = retweeted.withColumn(\"ratio_tweet_retweeted\", retweeted.retweeted / retweeted.tweets)\\\n",
    "                    .orderBy(\"ratio_tweet_retweeted\", ascending = 0)\n",
    "retweeted.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = retweeted.first()\n",
    "assert output.screen_name == 'PSOE' and output.friends_count == 13635 and output.tweets == 1 and output.followers_count == 671073 and output.ratio_tweet_retweeted == 155.0 and output.retweeted == 155, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 2:** Bases de datos HIVE y operaciones complejas\n",
    "\n",
    "Hasta ahora hemos estado trabajando con un pequeño sample de los tweets generados (el 0.1%). En esta parte de la PEC vamos a ver como trabajar y tratar con el dataset completo. Para ello vamos ha utilizar tanto transformaciones sobre tablas como operaciones sobre RDD cuando sea necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 2.1:** Bases de datos Hive\n",
    "\n",
    "Muchas veces los datos con los que vamos ha trabajar se van a utilizar en diversos proyectos. Una manera de organizar los datos es, en lugar de utilizar directamente los ficheros, recurrir a una base de datos para gestionar la información. En el entorno Hadoop una de las bases de datos más utilizadas es [Apache Hive](https://hive.apache.org/), una base de datos que permite trabajar con contenido distribuido.\n",
    "\n",
    "La manera de acceder a esta base de datos es creando un contexto Hive de manera muy similar a como declaramos un contexto SQL. Primero de todo vamos a declarar un variable ```hiveContext``` instanciándola como un objeto de la classe ```HiveContext```. Acto seguido vamos a comprobar cuantas tablas están registradas en este contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+\n",
      "|database|    tableName|isTemporary|\n",
      "+--------+-------------+-----------+\n",
      "| default|     hashtags|      false|\n",
      "| default| province_28a|      false|\n",
      "| default|           rt|      false|\n",
      "| default|         rt_t|      false|\n",
      "| default|   taula_nova|      false|\n",
      "| default|    tweets28a|      false|\n",
      "| default|    user_info|      false|\n",
      "| default|   users_agg6|      false|\n",
      "|        |tweets_sample|       true|\n",
      "|        |     user_agg|       true|\n",
      "+--------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hiveContext = HiveContext(sc)\n",
    "hiveContext.tables().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observad que ahora mismo tenemos cinco tablas registradas en este contexto. Tres de ellas no temporales y dos temporales, las que hemos registrado previamente. Por tanto sqlContext y hiveContext están concetados (es la misma sessión)\n",
    "\n",
    "Vamos ha crear una variable ```tweets``` que utilizaremos para acceder a la tabla ```tweets28a``` guardada en ```hiveContext``` utilizando para ello el método ```table()``` de este objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset contains 25419835 tweets\n"
     ]
    }
   ],
   "source": [
    "tweets = hiveContext.table(\"tweets28a\")\n",
    "print(\"Loaded dataset contains {} tweets\".format(tweets.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el mismo método que en el apartado 1.1, comprobad la estructura de la tabla que acabamos de cargar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: struct (nullable = true)\n",
      " |    |-- bounding_box: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- country_code: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- place_type: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- _id: string (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- friends_count: long (nullable = true)\n",
      " |    |    |-- id_str: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |    |    |-- statuses_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### **Parte 2.2:** Más allá de las transformaciones SQL\n",
    "\n",
    "Algunas veces vamos a necesitar obtener resultados que precisan operaciones que van más allá de lo que podemos conseguir utilizando el lenguaje SQL. En esta parte de la práctica vamos practicar cómo pasar de una tabla a un RDD, para hacer operaciones complejas, y luego volver a pasar a una tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "#### **Parte 2.2.1:** Tweets por población\n",
    "##### **Parte 2.2.1.1:** Utilizando SQL\n",
    "Un pequeño porcentaje, alrededor del 1%, de los tweets realizados está geolocalizado. Eso quiere decir que para estos tweets tenemos información acerca del lugar donde han sido realizados guardado en el campo ```place```. En este ejercicio se pide que utilizando una sentencia SQL mostréis en orden descendente cuántos tweets se han realizado en cada lugar. La tabla resultante ```tweets_place``` debe tener las siguientes columnas:\n",
    "- ***name:*** nombre del lugar\n",
    "- ***tweets:*** número de tweets\n",
    "\n",
    "Recordad que no todos los tweets en la base de datos tienen que tener información geolocalizada, tenéis que filtrarlos teniendo en cuenta todos los que tienen un valor no nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|       name|tweets|\n",
      "+-----------+------+\n",
      "|     Madrid| 19655|\n",
      "|  Barcelona| 13987|\n",
      "|    Sevilla|  3820|\n",
      "|   Valencia|  2833|\n",
      "|   Zaragoza|  2449|\n",
      "|Villamartín|  2364|\n",
      "|     Málaga|  2184|\n",
      "|     Murcia|  1800|\n",
      "|    Granada|  1637|\n",
      "|   Alicante|  1628|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_place = hiveContext.sql(\"SELECT place.name, count(_id) tweets FROM tweets28a WHERE place IS NOT NULL GROUP BY place.name ORDER BY tweets DESC\")\n",
    "tweets_place.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tweets_place.first()\n",
    "assert output.name == \"Madrid\" and output.tweets == 19655, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Parte 2.2.1.2:** Utilizando RDD\n",
    "\n",
    "Ahora se os pide que hagáis lo mismo pero esta vez utilizando RDD para realizar la agregación (recordad los ejercicios de contar palabras que hicisteis en la PEC 1).\n",
    "\n",
    "El primer paso consiste en generar un tabla ```tweets_geo``` que solo contenga información de tweets geolocalizados con una sola columna:\n",
    "- ***name:*** nombre del lugar desde donde se ha generado el tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|Las Palmas de Gra...|\n",
      "|Las Palmas de Gra...|\n",
      "|    Collado Villalba|\n",
      "|            Palencia|\n",
      "|               Egüés|\n",
      "|             Córdoba|\n",
      "|Castellón de la P...|\n",
      "| Granadilla de Abona|\n",
      "|San Vicente del R...|\n",
      "|              Madrid|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_geo = tweets.where(\"place IS NOT NULL\").select(\"place.name\")\n",
    "tweets_geo.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora viene la parte interesante. Una tabla puede convertirse en un RDD a través del atributo ```.rdd```. Este atributo guarda la información de la tabla en una lista donde cada elemento es un [objeto del tipo ```Row```](https://spark.apache.org/docs/1.1.1/api/python/pyspark.sql.Row-class.html). Los objetos pertenecientes a esta clase pueden verse como diccionarios donde la información de las diferentes columnas queda reflejada en forma de atributo. Por ejemplo, imaginad que tenemos una tabla con dos columnas, nombre y apellido, si utilizamos el atributo ```.rdd``` de dicha tabla obtendremos una lista con objetos del tipo row donde cada objeto tiene dos atributos: nombre y apellido. Para acceder a los atributos solo tenéis que utilizar la sintaxis *punto* de Python, e.g., ```row.nombre``` o ```row.apellido```.\n",
    "\n",
    "En esta parte del ejercicio se os pide que creeis un objeto ```tweets_lang_rdd``` que contenga una lista de tuplas con la información ```(name, tweets)``` sobre el nombre del lugar y el número de tweets generados desde allí. Recordad el ejercicio de contar palabras de la PEC 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_place_rdd = tweets_geo.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez generado este RDD vamos a crear un tabla. El primer paso es generar por cada tupla un objeto Row que contenga un atributo ```name``` y un atributo ```tweets```. Ahora solo tenéis que aplicar el método ```toDF()``` para generar una tabla. Ordenad las filas de esta tabla por el número de tweets en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|       name|tweets|\n",
      "+-----------+------+\n",
      "|     Madrid| 19655|\n",
      "|  Barcelona| 13987|\n",
      "|    Sevilla|  3820|\n",
      "|   Valencia|  2833|\n",
      "|   Zaragoza|  2449|\n",
      "|Villamartín|  2364|\n",
      "|     Málaga|  2184|\n",
      "|     Murcia|  1800|\n",
      "|    Granada|  1637|\n",
      "|   Alicante|  1628|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_place = tweets_place_rdd.toDF().select(\"name\").groupBy(\"name\")\\\n",
    "                            .agg({\"name\": \"count\"})\\\n",
    "                            .withColumnRenamed(\"count(name)\", \"tweets\")\\\n",
    "                            .orderBy(\"tweets\", ascending=0)\n",
    "\n",
    "tweets_place.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tweets_place.first()\n",
    "assert output.name == \"Madrid\" and output.tweets == 19655, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 2.2.2:** Contar hashtags\n",
    "\n",
    "En el ejercicio anterior hemos visto cómo podemos generar la misma información haciendo una agregación mediante SQL o utilizando RDDs. Como seguro que habéis observado la semántica de la sentencia SQL es mucho más limpia para realizar esta tarea. Pero no todas las tareas que os vais a encontrar se pueden hacer mediante sentencias SQL. En este ejercicio vamos a ver un ejemplo.\n",
    "\n",
    "El objetivo de este ejercicio es contar el número de veces que cada hashtag (palabras precedidas por un #) ha aparecido en el dataset. Para evitar la sobrerrepresentación debida a los retweets vamos a concentrarnos en solo aquellos tweets que no son retweets de ningún otro, o dicho de otra manera, en aquellos en los que el campo ```retweeted_status``` es nulo. Cread una variable ```non_retweets``` que contenga todos estos tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|@Pablo_Iglesias_ ...|\n",
      "|@josebouvila @Ada...|\n",
      "|— Mariano Rajoy ¿...|\n",
      "|Vamos a ver... SI...|\n",
      "|@albertoertoo Por...|\n",
      "|@FrancescFalip @A...|\n",
      "|Hey Vox, we just ...|\n",
      "|         sub: normal|\n",
      "|@JorgeBustos1 Tra...|\n",
      "|@sanchezdelreal @...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "non_retweets = tweets.where(\"retweeted_status IS NULL\").select(\"text\")\n",
    "non_retweets.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente vamos ha crear una variable ```hashtags``` que contenga una lista de tuplas con la información ```(hashtag, count)```. Para ello, cread un RDD que contenga una lista con el texto de todos los tweets. Una vez hecho este paso tenéis que extraer los hashtags (palabras precedidas por un #) y contarlos.\n",
    "\n",
    "Recordad los conocimientos adquiridos en la PEC 1 y el anterior ejercicio, os serán de gran ayuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#039', 278),\n",
       " ('#laEspañaquequiero', 2),\n",
       " ('#checknews', 1),\n",
       " ('#elizabethwarren', 6),\n",
       " ('#amrkplaying', 1),\n",
       " ('#BADAJOZ', 1),\n",
       " ('#ThisistheRealSpain', 1),\n",
       " ('#doodle', 3),\n",
       " ('#VotadInsensatos', 2),\n",
       " ('#ingresos', 4)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "import string\n",
    "\n",
    "#se creó para eliminar los emoticonos y caracteres extraños, pero con el regex utilizado no mejoraba el resultado:\n",
    "def removePunctuation(text):\n",
    "    no_emoticones= text.encode('ascii', 'ignore').decode('ascii')#eliminar emoticonos\n",
    "    no_puntuacion= re.sub(re.escape(string.punctuation), '', no_emoticones)#eliminar puntuación\n",
    "    no_espacio = text.replace(\"\\n\",\"\").replace(\"\\t\", \"\").replace(\"\\r\", \"\").strip()#eliminar espacios en blanco y saltos de linea\n",
    "    #mayusculas = no_espacio.upper()\n",
    "     \n",
    "    return no_espacio\n",
    "\n",
    "hashtags = non_retweets.rdd.flatMap(lambda x: re.findall(r\"(#\\w+)\", x.text)) \\\n",
    ".map(lambda x: (x, 1)).reduceByKey(add)\n",
    "\n",
    "hashtags.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se os pide que con el RDD obtenido generéis una tabla ```hashtagsTable``` compuesta de dos columnas:\n",
    "- ***hashtag***\n",
    "- ***num:*** número de veces que aparece cada hashtag.\n",
    "\n",
    "Ordenadla en orden descendente por número de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hashtag='#28A', num=158133),\n",
       " Row(hashtag='#ElDebateDecisivo', num=108607),\n",
       " Row(hashtag='#ELDEBATEenRTVE', num=94250),\n",
       " Row(hashtag='#EleccionesGenerales28A', num=33815),\n",
       " Row(hashtag='#EquiparacionYa', num=30567),\n",
       " Row(hashtag='#EleccionesL6', num=30075),\n",
       " Row(hashtag='#HazQuePase', num=26526),\n",
       " Row(hashtag='#DebateAtresmedia', num=21854),\n",
       " Row(hashtag='#DebateRTVE', num=17711),\n",
       " Row(hashtag='#LaHistoriaLaEscribesTú', num=16964)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "hashtagsTable = hashtags.toDF([\"hashtag\", \"num\"]).orderBy(\"num\", ascending=0)\n",
    "\n",
    "hashtagsTable.take(10)\n",
    "\n",
    "#El resultado obtenido difiere de 9 elementos y no pasa el test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Incorrect output",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-80fb386d50cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashtagsTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"#28A\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m158124\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Incorrect output\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Incorrect output"
     ]
    }
   ],
   "source": [
    "output = hashtagsTable.first()\n",
    "assert output.hashtag == \"#28A\" and output.num == 158124, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 3:** Sampling\n",
    "\n",
    "En muchas ocasiones, antes de lanzar costoso procesos, es una práctica habitual tratar con un pequeño conjunto de los datos para investigar algunas propiedades o simplemente para debugar nuestros algoritmos, a esta tarea se la llama sampling. En esta parte de la práctica vamos a ver los dos principales métodos de sampling y cómo utilizarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 3.1:** Homogeneo\n",
    "\n",
    "El primer sampling que vamos a ver es [el homogeneo](https://en.wikipedia.org/wiki/Simple_random_sample). Este sampling se basta en simplemente escoger una fracción de la población seleccionando aleatoriamente elementos de la misma.\n",
    "\n",
    "Primero de todo vamos ha realizar un sampling homogéneo del 1% de los tweets generados en periodo electoral sin reemplazo. Guardad en una variable ```tweets_sample``` este sampling utilizando el método ```sample``` descrito en la [API de pyspark SQL](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html). El seed que vais a utilizar para inicializar el generador aleatorio es 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets sampled: 254185\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "fraction = 0.01\n",
    "\n",
    "tweets_sample = tweets.sample(fraction, seed)\n",
    "\n",
    "print(\"Number of tweets sampled: {0}\".format(tweets_sample.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tweets_sample.count() == 254185, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las cosas que resulta interesante comprobar acerca de los patrones de uso de las redes sociales es el patrón de uso diario. En este caso nos interesa el número promedio de tweets que se genera cada hora del día. Para extraer esta información lo que haremos primero, será generar una tabla ```tweets_timestamp``` con la información:\n",
    "- ***created_at***: timestamp de cuando se publicó el tweet.\n",
    "- ***hour***: a que hora del dia corresponde.\n",
    "- ***day***: Fecha en formato MM-dd-YY\n",
    "\n",
    "La fecha que figura en la base de datos esta en la franja horaria GMT. El primer paso es pasar esta información al horario peninsular de España, podéis utilizar la función ```from_utc_timestamp``` para este fin. Así mismo, la función ```hour``` os servirá para extraer la hora del timestamp y la función ```date_format``` os permitirá generar la fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+--------+\n",
      "|         created_at|hour|     day|\n",
      "+-------------------+----+--------+\n",
      "|2019-04-21 02:24:26|   2|04-21-19|\n",
      "|2019-04-21 02:24:44|   2|04-21-19|\n",
      "|2019-04-21 02:24:46|   2|04-21-19|\n",
      "|2019-04-21 02:25:50|   2|04-21-19|\n",
      "|2019-04-21 02:25:53|   2|04-21-19|\n",
      "|2019-04-21 02:25:59|   2|04-21-19|\n",
      "|2019-04-21 02:26:21|   2|04-21-19|\n",
      "|2019-04-21 02:27:31|   2|04-21-19|\n",
      "|2019-04-21 02:28:01|   2|04-21-19|\n",
      "|2019-04-21 02:28:09|   2|04-21-19|\n",
      "|2019-04-21 02:28:14|   2|04-21-19|\n",
      "|2019-04-21 02:28:21|   2|04-21-19|\n",
      "|2019-04-21 02:28:35|   2|04-21-19|\n",
      "|2019-04-21 02:28:53|   2|04-21-19|\n",
      "|2019-04-21 02:29:10|   2|04-21-19|\n",
      "|2019-04-21 02:29:34|   2|04-21-19|\n",
      "|2019-04-21 02:29:39|   2|04-21-19|\n",
      "|2019-04-21 02:29:56|   2|04-21-19|\n",
      "|2019-04-21 02:30:06|   2|04-21-19|\n",
      "|2019-04-21 02:30:25|   2|04-21-19|\n",
      "+-------------------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, hour, from_utc_timestamp\n",
    "\n",
    "tweets_timestamp = tweets_sample.select( from_utc_timestamp(\"created_at\",\"GMT\").alias(\"created_at\"),hour('created_at').alias('hour'), date_format(\"created_at\",\"MM-dd-YY\").alias(\"day\"))\n",
    "                                \n",
    "\n",
    "tweets_timestamp.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso siguiente es agregar estos datos por hora y día en una tabla ```tweets_hour_day```. Tenéis que crear una tabla ```tweets_hour``` con la información:\n",
    "- ***hour:*** hora del dia\n",
    "- ***day:*** fecha\n",
    "- ***count:*** número de tweets generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+\n",
      "|     day|hour|count|\n",
      "+--------+----+-----+\n",
      "|04-23-19|  23| 4893|\n",
      "|04-22-19|  23| 4619|\n",
      "|04-28-19|  23| 3993|\n",
      "|04-24-19|   0| 3943|\n",
      "|04-22-19|  22| 3685|\n",
      "|04-23-19|  22| 3566|\n",
      "|04-28-19|  22| 3331|\n",
      "|04-29-19|   0| 3274|\n",
      "|04-23-19|   0| 2789|\n",
      "|04-28-19|  21| 1903|\n",
      "|04-24-19|   1| 1700|\n",
      "|04-28-19|  20| 1644|\n",
      "|04-28-19|  16| 1581|\n",
      "|04-28-19|  18| 1572|\n",
      "|04-29-19|   1| 1572|\n",
      "|04-28-19|  15| 1524|\n",
      "|04-28-19|  17| 1520|\n",
      "|04-28-19|  19| 1506|\n",
      "|04-28-19|  14| 1382|\n",
      "|04-24-19|  23| 1369|\n",
      "+--------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_hour_day = tweets_timestamp.groupBy(\"day\", \"hour\").agg({\"created_at\": \"count\"})\\\n",
    "                                    .withColumnRenamed(\"count(created_at)\", \"count\")\\\n",
    "                                    .orderBy(\"count\", ascending=0)\n",
    "\n",
    "#al no indicar orden el enunciado, le puse orderby count desc para ver que funciona.\n",
    "\n",
    "tweets_hour_day.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último solo nos queda hacer una agregación por hora para conseguir el promedio de tweets por hora. Tenéis que generar una tabla ```tweets_hour``` con la información:\n",
    "- ***hour:*** Hora\n",
    "- ***tweets:*** Promedio de tweets realizados\n",
    "\n",
    "Recordad que estamos trabajando con un sample del 1% por tanto tenéis que corregir la columna ```tweets``` para que refleje el promedio que deberíamos esperar en el conjunto completo de tweets. La tabla tiene que estar ordenada en orden ascendente de hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|hour|               avg|\n",
      "+----+------------------+\n",
      "|   0|1050.0555555555557|\n",
      "|   1| 517.6666666666666|\n",
      "|   2| 231.1578947368421|\n",
      "|   3|130.94117647058823|\n",
      "|   4| 93.70588235294117|\n",
      "|   5| 83.47058823529412|\n",
      "|   6|116.66666666666667|\n",
      "|   7|256.27777777777777|\n",
      "|   8| 425.2352941176471|\n",
      "|   9| 521.1111111111111|\n",
      "|  10| 583.4444444444445|\n",
      "|  11|             613.5|\n",
      "|  12| 621.8888888888889|\n",
      "|  13| 660.7777777777778|\n",
      "|  14| 678.5555555555555|\n",
      "|  15| 714.9444444444445|\n",
      "|  16| 677.9444444444445|\n",
      "|  17| 635.3888888888889|\n",
      "|  18| 675.5555555555555|\n",
      "|  19| 682.1111111111111|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_hour = tweets_hour_day.groupBy(\"hour\")\\\n",
    "                            .agg({\"count\": \"avg\"})\\\n",
    "                            .withColumnRenamed(\"avg(count)\", \"avg\")\\\n",
    "                            .orderBy(\"hour\", ascending=1)\n",
    "\n",
    "tweets_hour.limit(24).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, tenéis que producir un gráfico de barras utilizando Pandas donde se muestre la información que acabáis de generar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "result = tweets_hour.rdd.map(lambda p:(p.hour, p.avg)).collect()\n",
    "hours,avgs = zip(*result)\n",
    "\n",
    "plt.bar(hours, avgs)\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Average tweets')\n",
    "plt.title(\"Average tweets per hour 1%\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 3.2:** Estratificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En muchas ocasiones el sampling homogéneo no es adecuado ya que por la propia estructura de los datos determinados segmentos pueden estar sobrerrepresentadas. Este es el caso que observamos en los tweets donde las grandes áreas urbanas están sobrerepresentadas si lo comparamos con el volumen de población. En esta actividad vamos a ver cómo aplicar esta técnica al dataset de tweets, para obtener un sampling que respete la proporción de diputados por provincia.\n",
    "\n",
    "En España, el proceso electoral asigna un volumen de diputados a cada provincia que depende de la población y de un porcentaje mínimo asignado por ley. En el contexto Hive que hemos creado previamente (```hiveContext```) podemos encontrar una tabla (```province_28a```) que contiene información sobre las circunscripciones electorales. Cargad ésta tabla en una variable con nombre ```province```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------+----------+---------+\n",
      "|    capital|   province|              ccaa|population|diputados|\n",
      "+-----------+-----------+------------------+----------+---------+\n",
      "|     Teruel|     Teruel|            Aragón|     35691|        3|\n",
      "|      Soria|      Soria|   Castilla y León|     39112|        2|\n",
      "|    Segovia|    Segovia|   Castilla y León|     51683|        3|\n",
      "|     Huesca|     Huesca|            Aragón|     52463|        3|\n",
      "|     Cuenca|     Cuenca|Castilla-La Mancha|     54898|        3|\n",
      "|      Ávila|      Ávila|   Castilla y León|     57697|        3|\n",
      "|     Zamora|     Zamora|   Castilla y León|     61827|        3|\n",
      "|Ciudad Real|Ciudad Real|Castilla-La Mancha|     74743|        5|\n",
      "|   Palencia|   Palencia|   Castilla y León|     78629|        3|\n",
      "| Pontevedra| Pontevedra|           Galicia|     82802|        7|\n",
      "|     Toledo|     Toledo|Castilla-La Mancha|     84282|        6|\n",
      "|Guadalajara|Guadalajara|Castilla-La Mancha|     84910|        3|\n",
      "|      Ceuta|      Ceuta|             Ceuta|     85144|        1|\n",
      "|    Melilla|    Melilla|           Melilla|     86384|        1|\n",
      "|    Cáceres|    Cáceres|       Extremadura|     96098|        4|\n",
      "|       Lugo|       Lugo|           Galicia|     98025|        4|\n",
      "|     Girona|     Girona|          Cataluña|    100266|        6|\n",
      "|     Orense|     Orense|           Galicia|    105505|        4|\n",
      "|       Jaén|       Jaén|         Andalucía|    113457|        5|\n",
      "|      Cádiz|      Cádiz|         Andalucía|    116979|        9|\n",
      "+-----------+-----------+------------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "province = hiveContext.table(\"province_28a\")\n",
    "province.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer un sampling estratificado lo primero que tenemos que hacer es determinar la fracción que queremos asignar a cada categoría. En este caso queremos una fracción que haga que el ratio tweets diputado sea igual para todas las capitales de provincia. Tenemos que tener en cuenta que la precisión de la geolocalización en Twitter és normalmente a nivel de ciudad. Por eso, para evitar incrementar la complejidad del ejercicio, vamos a utilizar los tweets en capitales de provincia como proxy de los tweets en toda la provincia.\n",
    "\n",
    "Lo primero que tenéis que hacer es crear un tabla ```info_tweets_province``` que debe contener:\n",
    "- ***capital:*** nombre de la capital de provincia.\n",
    "- ***tweets:*** número de tweets geolocalizados en cada capital\n",
    "- ***diputados:*** diputados que asignados a la provincia.\n",
    "- ***ratio_tweets_diputado:*** número de tweets por diputado.\n",
    "\n",
    "Debéis ordenar la lista por ```ratio_tweets_diputado``` en orden ascendente.\n",
    "\n",
    "***Nota:*** Podéis realizar este ejercicio de muchas maneras, probablemente la más fácil es utilizar la tabla ```tweets_place``` que habéis generado en el apartado 2.2.1. Recordad cómo utilizar el ```join()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------+---------------------+\n",
      "|             capital|diputados|tweets|ratio_tweets_diputado|\n",
      "+--------------------+---------+------+---------------------+\n",
      "|              Teruel|        3|    35|   11.666666666666666|\n",
      "|          Pontevedra|        7|   154|                 22.0|\n",
      "|              Huesca|        3|    85|   28.333333333333332|\n",
      "|              Zamora|        3|    94|   31.333333333333332|\n",
      "|               Soria|        2|    79|                 39.5|\n",
      "|             Segovia|        3|   119|   39.666666666666664|\n",
      "|              Cuenca|        3|   146|   48.666666666666664|\n",
      "|               Cádiz|        9|   453|   50.333333333333336|\n",
      "|         Ciudad Real|        5|   276|                 55.2|\n",
      "|            Pamplona|        5|   281|                 56.2|\n",
      "|                Lugo|        4|   229|                57.25|\n",
      "|Santa Cruz de Ten...|        7|   471|    67.28571428571429|\n",
      "|                Jaén|        5|   356|                 71.2|\n",
      "|             Cáceres|        4|   288|                 72.0|\n",
      "|       San Sebastián|        6|   465|                 77.5|\n",
      "|              Toledo|        6|   494|    82.33333333333333|\n",
      "|             Badajoz|        6|   510|                 85.0|\n",
      "|         Guadalajara|        3|   264|                 88.0|\n",
      "|             Almería|        6|   529|    88.16666666666667|\n",
      "|            Albacete|        4|   371|                92.75|\n",
      "+--------------------+---------+------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_tweets_province=province.join(tweets_place, tweets_place.name == province.capital)\\\n",
    "                                .select(\"capital\",\"diputados\",\"tweets\")\\\n",
    "                                .withColumn(\"ratio_tweets_diputado\", tweets_place.tweets / province.diputados)\\\n",
    "                                .orderBy(\"ratio_tweets_diputado\", ascending = 1)\n",
    "\n",
    "info_tweets_province.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = info_tweets_province.first()\n",
    "maximum_ratio = floor(output.ratio_tweets_diputado * 100) / 100\n",
    "\n",
    "assert output.capital == \"Teruel\" and output.tweets == 35 and output.diputados == 3 and maximum_ratio == 11.66, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que vamos a necesitar es un diccionario con nombre ```ratios``` donde cada capital de provincia es una llave y su valor asociado es la fracción de tweets que vamos a samplear. En este caso lo que queremos es que el ratio de tweets por cada diputado sea similar para cada capital de provincia.\n",
    "\n",
    "Como queremos que el sampling sea lo más grande posible y no queremos que ninguna capital este infrarepresentada el ratio de tweets por diputado será el valor más pequeño podéis observar en la tabla ```info_tweets_province```, que corresponde a 11.66 tweets por diputado en Teruel. Tenéis este valor guardado en la variable ```maximum_ratio```.\n",
    "\n",
    "*Nota:* El método ```collectAsMap()``` transforma un PairRDD en un diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+------------------+\n",
      "|   capital|ratio_tweets_diputado|            ratios|\n",
      "+----------+---------------------+------------------+\n",
      "|    Teruel|   11.666666666666666|0.9994285714285714|\n",
      "|Pontevedra|                 22.0|              0.53|\n",
      "|    Huesca|   28.333333333333332|0.4115294117647059|\n",
      "+----------+---------------------+------------------+\n",
      "\n",
      "None\n",
      "0.9994285714285714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ratios_tweets = info_tweets_province.select(\"capital\",\"ratio_tweets_diputado\").withColumn(\"ratios\", maximum_ratio/info_tweets_province.ratio_tweets_diputado)\n",
    "print(ratios_tweets.limit(3).show())\n",
    "ratios = ratios_tweets.select(\"capital\", \"ratios\").rdd.collectAsMap()                   \n",
    "print(ratios[\"Teruel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generad una tabla ```geo_tweets``` con los tweets geolocalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_tweets = tweets.where(\"place IS NOT NULL\").select(\"place.name\", \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya estamos en disposición de hacer el sampling estratificado por población. Para ello podéis utilizar el método ```sampleBy()```. Utilizad 42 como seed del generador pseudoaleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                name|                text|\n",
      "+--------------------+--------------------+\n",
      "|Castellón de la P...|@orilatz @garcija...|\n",
      "|            Palencia|@CiudadanosCs @To...|\n",
      "|           Barcelona|Us juro que aques...|\n",
      "|            Alicante|@Trismegista Solo...|\n",
      "|              Cuenca|Clarinete, Sr. Ru...|\n",
      "|                León|📅 Hoy empezaremo...|\n",
      "|             Segovia|     #ApagaLaSexta #|\n",
      "|             Sevilla|   Manipulado.\n",
      "FAKE.|\n",
      "|            Valencia|¿Que no sabes si ...|\n",
      "|              Málaga|Marine Le Pen del...|\n",
      "|              Madrid|“¿De qué coño sab...|\n",
      "|               Cádiz|Si esta caradura ...|\n",
      "|           Barcelona|Yo aseguro que il...|\n",
      "|              Murcia|@Karma13355592 #H...|\n",
      "|             Córdoba|@gabrielrufian @I...|\n",
      "|          Pontevedra|@pablocasado_    ...|\n",
      "|                León|Que así sea !!! #...|\n",
      "|Santa Cruz de Ten...|Voy a usar las hi...|\n",
      "|              Murcia|Ya llega el desfi...|\n",
      "|                Lugo|@miquinta1 Se lla...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "sample = geo_tweets.sampleBy(\"name\", ratios, seed)\n",
    "sample.limit(20).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar el resultado del sampling vais a crear una tabla ```info_sample``` que contenga la siguiente información:\n",
    "- ***capital:*** nombre de la capital de provincia.\n",
    "- ***tweets:*** número de tweets sampleados en cada capital\n",
    "- ***diputados:*** diputados que asignados a la provincia.\n",
    "- ***ratio_tweets_diputado:*** número de tweets por diputado.\n",
    "\n",
    "Ordenad la tabla resultante por orden de ```ratio_tweets_diputado```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+------+---------------------+\n",
      "|        capital|diputados|tweets|ratio_tweets_diputado|\n",
      "+---------------+---------+------+---------------------+\n",
      "|        Melilla|        1|     6|                  6.0|\n",
      "|          Ceuta|        1|     8|                  8.0|\n",
      "|    Ciudad Real|        5|    44|                  8.8|\n",
      "|           León|        4|    38|                  9.5|\n",
      "|       Albacete|        4|    39|                 9.75|\n",
      "|         Bilbao|        8|    79|                9.875|\n",
      "|         Zamora|        3|    30|                 10.0|\n",
      "|    Guadalajara|        3|    30|                 10.0|\n",
      "|       Palencia|        3|    30|                 10.0|\n",
      "|       Alicante|       12|   122|   10.166666666666666|\n",
      "|      Tarragona|        6|    62|   10.333333333333334|\n",
      "|           Lugo|        4|    42|                 10.5|\n",
      "|        Segovia|        3|    32|   10.666666666666666|\n",
      "|         Toledo|        6|    64|   10.666666666666666|\n",
      "|       Pamplona|        5|    54|                 10.8|\n",
      "|           Jaén|        5|    54|                 10.8|\n",
      "|         Girona|        6|    65|   10.833333333333334|\n",
      "|         Madrid|       37|   402|   10.864864864864865|\n",
      "|Vitoria-Gasteiz|        4|    44|                 11.0|\n",
      "|      Santander|        5|    55|                 11.0|\n",
      "+---------------+---------+------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "info_sample = province.join(sample, sample.name == province.capital)\\\n",
    "                                .select(\"capital\",\"diputados\")\\\n",
    "                                .groupBy(\"capital\", \"diputados\")\\\n",
    "                                .agg({\"capital\": \"count\"})\\\n",
    "                                .withColumnRenamed(\"count(capital)\", \"tweets\")\n",
    "\n",
    "info_sample = info_sample.withColumn(\"ratio_tweets_diputado\", info_sample.tweets / province.diputados)\\\n",
    "                                .orderBy(\"ratio_tweets_diputado\", ascending = 1)\n",
    "                                \n",
    "\n",
    "info_sample.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = info_sample.first()\n",
    "assert output.capital == \"Melilla\" and output.tweets == 6 and output.diputados == 1 and output.ratio_tweets_diputado == 6.0, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como veis el sampling no es exacto, es una aproximación. Pero como podéis imaginar acercar el sampling a la representatividad electoral de las regiones son necesarios en muchos análisis.\n",
    "\n",
    "Para comprobarlo contad primero todos los hashtags presentes en la tabla ```geo_tweets``` tal como hemos hecho en el apartado 2.2.2 y ordenad el resultado por número de tweets en orden descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#26Abril', 7),\n",
       " ('#EscribamosLaHistoria', 1),\n",
       " ('#CamíALaRepública', 1),\n",
       " ('#UnMundoDeSensaciones', 1),\n",
       " ('#Bcn', 1),\n",
       " ('#Pedagogía', 1),\n",
       " ('#elche', 1),\n",
       " ('#MESA', 1),\n",
       " ('#porlosderechosdeTODOS', 1),\n",
       " ('#TaxiEnLucha', 8)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hashtags_geo = geo_tweets.rdd.flatMap(lambda x: re.findall(r\"(#\\w+)\", x.text)) \\\n",
    ".map(lambda x: (x, 1)).reduceByKey(add)\n",
    "\n",
    "hashtags_geo.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparad este resultado con el que obtenemos cuando creamos una tabla ```hashtagsTable_sample``` donde contamos los hashtags en el sample. Ordenad la tabla por número de tweets en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|             hashtag| num|\n",
      "+--------------------+----+\n",
      "|                #28A|7417|\n",
      "|   #ElDebateDecisivo|3942|\n",
      "|     #ELDEBATEenRTVE|3514|\n",
      "|     #EquiparacionYa|2309|\n",
      "|       #6AbrilMadrid|1839|\n",
      "|         #ILPJusapoL|1762|\n",
      "|         #HazQuePase|1622|\n",
      "|#EleccionesGenera...|1478|\n",
      "|       #EleccionesL6| 988|\n",
      "|        #ValorSeguro| 864|\n",
      "|   #DebateAtresmedia| 847|\n",
      "|#LaHistoriaLaEscr...| 752|\n",
      "|           #VotaPSOE| 701|\n",
      "|         #DebateRTVE| 699|\n",
      "| #LaEspañaQueQuieres| 658|\n",
      "|#EleccionesGenerales| 653|\n",
      "|    #VamosCiudadanos| 598|\n",
      "|           #DebatTV3| 597|\n",
      "|             #España| 580|\n",
      "|            #28Abril| 571|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashtagsTable_sample = hashtags_geo.toDF([\"hashtag\", \"num\"]).orderBy(\"num\", ascending=0)\n",
    "hashtagsTable_sample.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Parte 4:** Introducción a los datos relacionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El hecho de trabajar con una base de datos que contiene información generada en una red social nos permite introducir el concepto de datos relacionales. Podemos definir datos relacionales como aquellos en los que existen relaciones entre las entidades que constituyen la base de datos. Si estas relaciones son binarias, relaciones 1 a 1, podemos representar las relaciones como un grafo compuesto por un conjunto de vértices $\\mathcal{V}$ y un conjunto de aristas $\\mathcal{E}$ que los relacionan.\n",
    "\n",
    "En el caso de grafos que emergen de manera orgánica, este tipo de estructura va más allá de los grafos regulares que seguramente conocéis. Este tipo de estructuras se conocen como [redes complejas](https://es.wikipedia.org/wiki/Red_compleja). El estudio de la estructura y dinámicas de este tipo de redes ha contribuido a importantes resultados en campos tan dispares como la física, la sociología, la ecología o la medicina.\n",
    "\n",
    "![complex_network](https://images.squarespace-cdn.com/content/5150aec6e4b0e340ec52710a/1364574727391-XVOFAB9P6GHKTDAH6QTA/lastfm_800_graph_white.png?content-type=image%2Fpng)\n",
    "\n",
    "En esta última parte de la práctica vamos ha trabajar con este tipo de datos. En concreto vamos a modelar uno de los posibles relaciones presentes en el dataset, la red de retweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 4.1:** Generar la red de retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 4.1.1**: Construcción de la edgelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero se os pide es que generéis la red. Hay diversas maneras de representar una red compleja, por ejemplo, si estuvierais interesados en trabajar en ellas desde el punto de vista teórico, la manera más habitual de representarlas es utilizando una [matriz de adyacencia](https://es.wikipedia.org/wiki/Matriz_de_adyacencia). En esta práctica vamos a centrarnos en el aspecto computacional, una de las maneras de mas eficientes (computacionalmente hablando) de representar una red es mediante su [*edge list*](https://en.wikipedia.org/wiki/Edge_list), una tabla que especifica la relación a parejas entre las entidades.\n",
    "\n",
    "Las relaciones pueden ser bidireccionales o direccionales y tener algún peso asignado o no (weighted or unweighted). En el caso que nos ocupa, estamos hablando de una red dirigida, un usuario retuitea a otro, y podemos pensarla teniendo en cuenta cuántas veces esto ha pasado.\n",
    "\n",
    "Lo primero que haréis para simplificar el cómputo,  es crear un sample homogéneo sin reemplazo del 1% de los tweets. Utilizando los conocimientos que habéis aprendido en el apartado 3.1. Utilizaremos 42 como valor para la seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "fraction = 0.01\n",
    "sample = tweets.sample(False, fraction, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vais a crear una tabla ```edgelist``` con la siguiente información:\n",
    "- ***src:*** usuario que retuitea\n",
    "- ***dst:*** usuario que es retuiteado\n",
    "- ***weight:*** número de veces que un usuario retuitea a otro.\n",
    "\n",
    "Filtrar el resultado para que contenga sólo las relaciones con un weight igual o mayor a dos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5247 edges on the network.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "edgelist= sample.where(\"retweeted_status IS NOT NULL\").select( col(\"user.screen_name\").alias(\"src\"),col(\"retweeted_status.user.screen_name\").alias(\"dst\"))\\\n",
    ".groupBy(\"src\",\"dst\").agg({\"dst\": \"count\"})\\\n",
    ".withColumnRenamed(\"count(dst)\", \"weight\")\n",
    "\n",
    "edgelist = edgelist.where(\"weight >1\")\n",
    "#edgelist.limit(20).show()\n",
    "L = edgelist.count()\n",
    "\n",
    "print(\"There are {0} edges on the network.\".format(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert L == 5247, \"Incorrect ouput\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 4.1.2:** Centralidad de grado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los descriptores más comunes en el análisis de redes es el grado. El grado cuantifica cuántas aristas están conectadas a cada vértices. En el caso de redes dirigidas como la que acabamos de crear este descriptor está descompuesto en el:\n",
    "- **in degree**: cuantas aristas apuntan al nodo\n",
    "- **out degree**: cuantas aristas salen del nodo\n",
    "\n",
    "Si haces un ranquing de estos valores vais a obtener medida de centralidad, la [centralidad de grado](https://en.wikipedia.org/wiki/Centrality#Degree_centrality), de cada uno de los nodos.\n",
    "\n",
    "Se os pide que generéis una tabla con la información:\n",
    "- ***screen_name:*** nombre del usuario.\n",
    "- ***outDegree:*** out degree del nodo.\n",
    "\n",
    "Ordenado la tabla por out degree en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|    screen_name|outDegree|\n",
      "+---------------+---------+\n",
      "|   rosavergar23|       11|\n",
      "|JulioAl18175505|       10|\n",
      "|      el_partal|       10|\n",
      "|    SSarelvis67|        9|\n",
      "|Teresaperezcep1|        8|\n",
      "|miguelgutiperez|        8|\n",
      "|       anap1958|        8|\n",
      "|      MACUBERNA|        7|\n",
      "|  yomismaconcha|        7|\n",
      "|        Fermirv|        7|\n",
      "|    pacomarina6|        7|\n",
      "|   Socialista60|        7|\n",
      "|     astroman78|        7|\n",
      "|       jasalo54|        7|\n",
      "|  Rafa_eltorete|        7|\n",
      "|        lyuva26|        7|\n",
      "|    mercedescdz|        6|\n",
      "|        crg1212|        6|\n",
      "|  joanagabarrof|        6|\n",
      "|     carrasquem|        6|\n",
      "+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outDegree = edgelist.select(\"src\", \"weight\").groupBy(\"src\").agg({\"weight\": \"count\"})\\\n",
    ".withColumnRenamed(\"count(weight)\", \"outDegree\")\\\n",
    ".withColumnRenamed(\"src\", \"screen_name\")\\\n",
    ".orderBy(\"outDegree\", ascending=0)\n",
    "\n",
    "outDegree.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = outDegree.first()\n",
    "assert output.screen_name == \"rosavergar23\" and output.outDegree == 11, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se os pide ahora que generéis una tabla con la información:\n",
    "- ***screen_name:*** nombre del usuario.\n",
    "- ***inDegree:*** in degree del nodo.\n",
    "\n",
    "Ordenad la tabla por in degree en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|    screen_name|inDegree|\n",
      "+---------------+--------+\n",
      "|         vox_es|     330|\n",
      "|   ahorapodemos|     279|\n",
      "|           PSOE|     242|\n",
      "|   CiudadanosCs|     218|\n",
      "|  Santi_ABASCAL|     163|\n",
      "|      populares|     119|\n",
      "|Pablo_Iglesias_|     109|\n",
      "|  AlbanoDante76|      95|\n",
      "|Front_Republica|      89|\n",
      "|           KRLS|      86|\n",
      "|sanchezcastejon|      79|\n",
      "|      JuntsXCat|      70|\n",
      "|       iescolar|      66|\n",
      "|         boye_g|      55|\n",
      "| AntonioMaestre|      52|\n",
      "|   pablocasado_|      48|\n",
      "|       ivanedlm|      46|\n",
      "| hermanntertsch|      42|\n",
      "|    CastigadorY|      40|\n",
      "|     eldiarioes|      37|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inDegree = edgelist.select(\"dst\", \"weight\").groupBy(\"dst\").agg({\"weight\": \"count\"})\\\n",
    ".withColumnRenamed(\"count(weight)\", \"inDegree\")\\\n",
    ".withColumnRenamed(\"dst\", \"screen_name\")\\\n",
    ".orderBy(\"inDegree\", ascending=0)\n",
    "\n",
    "inDegree.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = inDegree.first()\n",
    "assert output.screen_name == \"vox_es\" and output.inDegree == 330, \"Incorrect output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### **Part 4.2:** Graphframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de estructuras es muy común en muchos datasets y su análisis cada vez se ha vuelto más habitual. Para simplificar las operaciones y el análisis vamos a utilizar una librería específicamente diseñada para trabajar en redes en sistemas distribuidos: [**Graphframes**](https://graphframes.github.io/graphframes/docs/_site/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "pyfiles = str(sc.getConf().get(u'spark.submit.pyFiles')).split(',')\n",
    "sys.path.extend(pyfiles)\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 4.2.1:** Crear un graph frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que vamos ha hacer es crear un objeto ```GraphFrame``` que contendrà toda la información de la red.\n",
    "\n",
    "En un paso previo ya hemos creado la *edge list* ahora vamos a crear una lista con los vértices. Crear una tabla ```vértices``` que contenga una única columna ```id``` con los nombre de usuario de todos los vértices. Recordad que hay vértices que puede que solo tengan aristas incidentes y otros que puede que no tengan (tenéis que utilizar la información de ambas columnas de la ```edgelist```). Recordad que la lista de vértices es un conjunto donde no puede haber repetición de identificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5111 nodes on the network.\n"
     ]
    }
   ],
   "source": [
    "vertices = edgelist.select(col(\"src\").alias(\"id\")).union(edgelist.select(col(\"dst\").alias(\"id\"))).distinct()\n",
    "N = vertices.count()\n",
    "print(\"There are {0} nodes on the network.\".format(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert N == 5111, 'Incorrect output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con las aristas, podéis asignar atributos a los vértices. Completad la tabla ```vertices``` haciendo un *inner join* por ```id``` con la tabla ```user_info``` guardada en el contexto ```hiveContext```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+------+------------+---------+---------+\n",
      "|             id|lang|tweets|total_tweets|following|followers|\n",
      "+---------------+----+------+------------+---------+---------+\n",
      "|     __akasha__|  es|   126|        5225|      576|      180|\n",
      "|     NoEsPotCat|  ca|   126|       13294|     1163|     6076|\n",
      "|      elisabeni|  es|    95|       89330|     1170|   110432|\n",
      "| GirautaOficial|  es|   190|       17293|     1960|    97179|\n",
      "|   Asturgalicia|  es|  2137|      100752|     5002|     2976|\n",
      "|    voxpalencia|  es|   260|        4322|      540|     1662|\n",
      "|DavidMa33922012|  es|   516|        5954|      231|      174|\n",
      "|       CsFerrol|  es|   317|       15464|      936|     2193|\n",
      "|    besoviajero|  es|  1494|       57882|     1372|     1397|\n",
      "|      Montsecs3|  en|    83|         148|       58|        1|\n",
      "|AnaFern52249672|  es|  1468|       73972|       31|      198|\n",
      "|IsadoraArtemisa|  es|   265|        5211|      630|      440|\n",
      "|       alrolsan|  es|   306|       14053|     3023|     2200|\n",
      "|     SaezEuropa|  es|   271|        9993|       96|      174|\n",
      "|      CoucouVic|  ca|   703|       39346|     6485|    11280|\n",
      "|BernaldoDQuiros|  es|   748|       43872|      176|     5972|\n",
      "|     KaliYuga13|  ca|  1129|      187250|      328|      601|\n",
      "| labib_martinez|  es|   304|       11177|      282|      113|\n",
      "| giuliano197410|  es|   434|        5246|      418|      343|\n",
      "|   ejcualquiera|  es|   546|       14216|     2949|     1885|\n",
      "+---------------+----+------+------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_info =  hiveContext.table(\"user_info\")\n",
    "vertices = vertices.join(user_info, \"id\")\n",
    "\n",
    "vertices.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos la edgelist y la lista de edges estamos en disposición de instanciar [un objecto ```GraphFrame```](https://graphframes.github.io/graphframes/docs/_site/api/python/graphframes.html). Instanciad este objeto en la variable ```network```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "network = GraphFrame(vertices, edgelist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto que acabais de crear tiene muchas atributos y métodos para el analisis de redes [(comprobad el API)](https://graphframes.github.io/graphframes/docs/_site/api/python/graphframes.html). Se os pide que utilizéis el atributo ```inDegrees``` para, conjuntamente con la transformación ```orderBy```, mostrar la informació del in degree en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|             id|inDegree|\n",
      "+---------------+--------+\n",
      "|         vox_es|     330|\n",
      "|   ahorapodemos|     279|\n",
      "|           PSOE|     242|\n",
      "|   CiudadanosCs|     218|\n",
      "|  Santi_ABASCAL|     163|\n",
      "|      populares|     119|\n",
      "|Pablo_Iglesias_|     109|\n",
      "|  AlbanoDante76|      95|\n",
      "|Front_Republica|      89|\n",
      "|           KRLS|      86|\n",
      "|sanchezcastejon|      79|\n",
      "|      JuntsXCat|      70|\n",
      "|       iescolar|      66|\n",
      "|         boye_g|      55|\n",
      "| AntonioMaestre|      52|\n",
      "|   pablocasado_|      48|\n",
      "|       ivanedlm|      46|\n",
      "| hermanntertsch|      42|\n",
      "|    CastigadorY|      40|\n",
      "|     eldiarioes|      37|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calculated_indegree= network.inDegrees.orderBy(\"inDegree\", ascending=0)\n",
    "calculated_indegree.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haced lo mismo con el atributo ```outDegrees``` para, conjuntamente con la transformación ```orderBy```, mostrar la informació del out degree en orden descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|             id|outDegree|\n",
      "+---------------+---------+\n",
      "|   rosavergar23|       11|\n",
      "|JulioAl18175505|       10|\n",
      "|      el_partal|       10|\n",
      "|    SSarelvis67|        9|\n",
      "|Teresaperezcep1|        8|\n",
      "|miguelgutiperez|        8|\n",
      "|       anap1958|        8|\n",
      "|      MACUBERNA|        7|\n",
      "|  yomismaconcha|        7|\n",
      "|        Fermirv|        7|\n",
      "|    pacomarina6|        7|\n",
      "|   Socialista60|        7|\n",
      "|     astroman78|        7|\n",
      "|       jasalo54|        7|\n",
      "|  Rafa_eltorete|        7|\n",
      "|        lyuva26|        7|\n",
      "|    mercedescdz|        6|\n",
      "|        crg1212|        6|\n",
      "|  joanagabarrof|        6|\n",
      "|     carrasquem|        6|\n",
      "+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calculated_outdegree= network.outDegrees.orderBy(\"outDegree\", ascending=0)\n",
    "calculated_outdegree.limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parte 4.2.2:** Centralidad PageRank\n",
    "\n",
    "Hasta ahora hemos visto uno de los descriptores más básicos del análisis de redes, la centralidad de grado. Ahora vamos a aprovechar las funcionalidades de GraphFrames para estudiar [la centralidad del *PageRank*](https://en.wikipedia.org/wiki/PageRank), el algoritmo original que utilizaba Google para indexar la web.\n",
    "\n",
    "La idea detrás de este algoritmo es representar la reputación. Google pensaba que no solo es importante saber cuántos enlaces apuntan a una web, sino también su importancia. Para analizarlo crearon este algoritmo que básicamente queda formalmente representado por:\n",
    "$$\n",
    "\\mbox{PR}(p_i) = \\frac{1 - d}{N} + d \\sum_{p_j \\in M(p_i)}\\frac{\\mbox{PR}(p_j)}{L(p_j)}\n",
    "$$\n",
    "Donde $\\mbox{PR}(p_i)$ representa el PageRank de la página $p_i$, $d$ es un factor de amortiguación, $p_j$ es una página que enlaza $p_i$ y $L(p_j)$ es el numero total de enlaces salientes de la página $j$. Como podéis ver es un algoritmo recursivo, que se puede resolver de diferentes maneras.\n",
    "\n",
    "Afortunadamente, no tendréis que preocuparos por la implementación ya que la classe GraphFrames implementa un método ```pageRank``` para calcularlo. Cread una tabla ```page_rank``` con los resultados. Mostrad los resultados con el ```id``` del nodo y su indice ```PageRank``` en orden descendente\n",
    "\n",
    "- ***Nota 1:*** Utilizando los parametros del metodo tenéis que fijar la probabilidad de reinicio a 0.15 y el numero máximo de iteraciones a 5\n",
    "- ***Nota 2:*** El tiempo de cómputo puede oscilar de 3 a 20 minutos dependiendo de la carga del servidor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_rank = network.pageRank(resetProbability=0.15, maxIter=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|             id|          pagerank|\n",
      "+---------------+------------------+\n",
      "|         vox_es|170.31097167053235|\n",
      "|Pablo_Iglesias_|120.80247412776232|\n",
      "|  Santi_ABASCAL|102.41446139478607|\n",
      "|   ahorapodemos| 100.2837043688577|\n",
      "| hermanntertsch|  95.4113047882803|\n",
      "|   CiudadanosCs|   93.245766612301|\n",
      "|           PSOE| 91.13458970143508|\n",
      "|  InesArrimadas|47.359707863261676|\n",
      "| RaiLopezCalvet| 47.34108812182462|\n",
      "|  Albert_Rivera| 47.31355661690909|\n",
      "|        ehbildu| 44.45332094685276|\n",
      "|      populares|41.608475611135525|\n",
      "|  AlbanoDante76| 34.74766697076182|\n",
      "|Front_Republica| 33.79746007120636|\n",
      "|Front_Republica| 33.79746007120636|\n",
      "|        Jjsb441|31.125164754674714|\n",
      "|           KRLS| 30.45209286696975|\n",
      "|sanchezcastejon| 27.33777199805432|\n",
      "|      JuntsXCat| 26.74753185335384|\n",
      "|       iescolar|24.662781385904097|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page_rank.vertices.select(\"id\",\"pagerank\").orderBy(\"pagerank\", ascending=0).limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Observáis alguna diferencia con los resultados de importancia del out degree?¿A que creéis que se debe?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "El orden de los ids es ligeramente diferentes, además el orden del valor es diferente, en la tabla de out degree vox_es tiene un valor de 11, pero en este último resultado un pagerank de 170."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
